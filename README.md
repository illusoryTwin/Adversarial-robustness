# Adversarial Robustness

The study is based on the tutorial - "Adversarial Robustness - Theory and Practice".

In the `reports` folder, you can find a synopsis of the article. All the testing code is located in the `src` folder.


The study focuses on adversarial attacks and the subsequent adversarial robustness, which pose significant threats to machine learning models.

## Introduction


Adversarial attacks are small yet malicious perturbations applied to input datasets. Often invisible to the human eye, they can damage the internal structure of the model and consequently its accuracy and reliability. These attacks pose a threat to security-critical tasks, including computer vision and natural language processing, among others.

