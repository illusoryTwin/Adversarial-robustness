\section{Chapter 1 - Introduction to adversarial robustness}
Let's define the model for a classification problem with the hypothesis function $h_\Theta$.  The loss function (softmax loss) of the model is the following:
$l(h_{\theta}(x_{i}), y_{i})$.




The common approach for the classification problem is to solve the optimization problem:
\[\min_{\theta} \frac{1}{m} \sum_{i=1}^{m} l(h_{\theta}(x_{i}), y_{i})\]

It is typically solved by calculating the gradient of our loss function:
\[ \Theta := \Theta - \dfrac{\alpha}{\beta} \sum_{i \in \beta} \nabla_\Theta l (h_{\theta}(x_{i}), y_{i}) \]

In standard optimization tasks, we aim to minimize the loss function. However, for creating adversarial attacks, our objective is to maximize it:
\[\max_{\hat{x}} l(h_{\Theta}(\hat{x}, y))\]

$\hat x$ here describes the adversarial example.

What we are specifically interested in is the gradient of the loss function:
\[  \nabla_\Theta l (h_{\theta}(x_{i}), y_{i}) \]

Adversarial example $\hat{x}$ needs to be similar to the original input $x$ to stay meaningful, so we optimize over the pertrubation to x.
\[\max_{\delta \in \Delta} l (h_{\Theta}(x+\delta), y)\]

For intuition, $\Delta$ should be the range in which the input is the same as the original $x$.


\subsection{Targeted attacks}
Using this technique we can deceive the model into predicting an incorrect class. To achieve this, we aim to maximize the loss function for the correct label while minimizing the loss 
function for the targeted class. 

\[\text{maximize} \left( l(h_{\theta}(x + \delta), y) - l(h_{\Theta}(x + \delta), y_{\text{target}}) \right)
\]


\subsection{Adversarial risks}

The concept of adversarial risk is introduced to enhance the robustness of the model. We can evaluate adversarial risks alongside traditional empirical risks. They are estimated using a finite set of data.
\[ R_{adv}(h\theta) = E_{(x,y) \sim D} \left[ \max_{\delta \in \Delta(x)} \mathcal{L}(h\theta(x+\delta),y) \right] \]

Evaluating adversarial risks allows us to assess the accuracy of the model even in cases where it is subjected to attacks or adversarial manipulation. This evaluation can help us predict
the behaviour of our model in case of attacks. 

\[ \Theta := \Theta - \dfrac{\alpha}{|\beta|} \sum_{(x, y) \in \beta} \nabla_\Theta \max_{\delta \in \Delta(x)} l (h_{\theta}(x_{i}+\delta), y_{i}) \]


The gradient of the inner term, which involves a maximization problem, is computed as follows, taking into account Danskin's theorem.% \[\delta* = argmax_\delta \in \Delta(x) l (h_\Theta (x + \delta*), y)\]

\[\delta^* = \text{argmax}_{\delta \in \Delta(x)} \mathcal{L}(h_{\Theta}(x + \delta^*), y)\]

\[\Delta_\Theta \max_{\delta \in \Delta(x)} l (h_\Theta(x+\delta), y) = \Delta_\Theta l (h_\Theta(x + \delta^*), y)\]