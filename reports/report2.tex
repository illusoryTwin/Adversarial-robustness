\section{Chapter 2 - Linear models}

\subsection{Linear models}

Let's define the model for a binary classification problem with the hypothesis function $h_\Theta$.  The loss function (logistic loss) of the model is the following:

The hypothesis function is:

\[h_\Theta(x) = w^T x + b\]

And the loss function is:

\[ l(h_\Theta(x), y) = \log(1 + \exp(-y \cdot h_\Theta(x))) \equiv L(y \cdot h_\Theta(x)), \]
where $L(z)=\log(1+\exp(-z))$, a decreasing function.

\[\max_{\|\delta\| \leq \epsilon} l(w^T (x+\delta), y) \equiv \max_{\|\delta\| \leq \epsilon} L(y \cdot (w^T(x+\delta)) + b)\]

Due to $L$ being a decreasing function, its argument must be minimized in order for the function to reach its maximum. 

\[ \max_{\|\delta\| \leq \epsilon} L(y \cdot (w^T(x+\delta)) + b) = L \min_{\|\delta\| \leq \epsilon}(y \cdot (w^T(x+\delta)) + b) \equiv y \cdot (w^T(x + b)) + \min_{\|\delta\| \leq \epsilon}(y \cdot w^T \cdot \delta)\]

For solving this optimization problem: \[\min_{\|\delta\| \leq \epsilon}(y \cdot w^T \cdot \delta),\] let's first assume
that $y=1$. 

Since $ | \delta |_{\infty} \leq \epsilon $, where $\epsilon > 0$, the obvious approach to solve the given minimization problem is to assign $\delta = -\epsilon$ for $w^T \geq 0$ and $\delta = \epsilon$ for $w^T \leq 0$.

\[\delta^*=-y\epsilon \cdot \text{sign}(w)\]

Let's denote the optimized $\delta^*$ as \[\delta^* = -\epsilon ||w||_1\]
and substitute it in the initial equation, so that:

\[\max_{\|\delta\|_{\infty} \leq \epsilon} L(y \cdot (w^T(x+\delta)+b)) = L(y \cdot (w^Tx+b) - \epsilon \|w\|_1)\]

\subsection{Practical results}
I observed a tradeoff while testing the robustness of the model on MNIST dataset for images labeled as 0 and 1. Surprisingly, the model made more mistakes on non-adversarial data, despite previously performing better on this type of data.