\section{Chapter 3 - Neural Networks}

\subsection{Aspects of Neural Networks}

The application and performance of adversarial attacks are highly relevant to neural networks.

The form of the optimization problem (inner maximization problem) remains the same:
\[\max_{\|\delta\|\leq\epsilon} \ell(h_{\theta}(x + \delta), y)\]    
What is different from the previous examples is $h(\theta)$, which now represents a neural network.

The complexity of neural networks' architecture makes them more challenging to be robust, while simultaneously rendering them susceptible to adversarial attacks.

First of all, loss surfaces do not often guide to the optimal solution, while they can be too steep, which often corresponds to local optima convergence.

Secondly, the inner maximization problem for neural networks is also more challenging due to the non-convexity of the cost surface.

Moreover, it is more difficult to navigate the cost surface.\\

To further explore the topic of adversarial attacks in neural networks, let's propose that adversarial attacks are based on two main components:

\begin{enumerate}
\item The norm of the perturbation ball.
\item The optimization method used within that norm ball.\\
\end{enumerate}

Three main approaches to adversarial attacks on neural networks exist: 
\begin{enumerate}
    \item Lower Bounding the inner optimization, 
    \item Exactly solving the inner maximization (combinatorial optimization), 
    \item Upper Bound optimization
\end{enumerate}


\subsection{Lower Bounding the inner optimization}

The Lower Bound method relying on empirical solutions is considered the most common approach.

\subsubsection{The Fast Gradient Sign Method (FGSM)}

The idea of adversarial attacks via the FGSM relies on calculating an optimal perturbation 
$\delta$ that maximizes the loss function. This calculation is based on the gradient obtained through backpropagation. 
This gradient indicates how the loss function changes when we make small adjustments to $\delta$.

Based on the gradient, we adjust $\delta$ to maximize the loss function. 
\[g = \nabla_\delta l(h_\theta(x + \delta), y)\]

This is performed by adding a fraction of 
the gradient to $\delta$, scaled by a small step size parameter $\alpha$.

\[\delta = \delta +  \alpha \cdot g\]

We adjust $\delta$ based on the sign of the gradient: when $g<0$, $\delta < 0$ and when $g>0$, $\delta > 0$. 
Thus, we have \[\delta := \epsilon \cdot \text{sign}(g)\].

However, we also need to stick to the constraints. So, we might need to project $\delta$ back into a feasible region after adjusting it.

The  $l_{\infty}$ norm ball, which selects the maximum absolute value among the elements in the set, is well-suited for this task.

We must ensure $\delta$ remains within the norm ball: $||\delta||_\infty \leq \epsilon$. 
Therefore, we project $\delta$ back into the norm ball after each adjustment.


So, then we project $\delta$ again into the norm ball.  


FGSM is specifically designed to attack under $l_\infty$ norm. 


\subsubsection{Projected Gradient Descent}

An iterative method that ensures to adjust the perturbation $\delta$ in the direction that maximizes the loss function 
while ensuring it stays within the specified constraints.

Both FGSM and PGD generate adversarial examples that are optimal since they maximize the loss function.
While FGSM is a straightforward, one-step process, which takes a single step in the direction of the goal,
PGD iteratively updates the perturbation to generate adversarial examples. 

Thus, we can assume PGD as an iterative FGSM, which iterates over multiple steps.

\[\delta := \text{P}(\delta + \alpha \nabla_{\delta} \ell(h_\theta(x + \delta), y))\]

$P$ is the projection of the value onto the ball norm.

However, PGD is not proved to be more effective than FGSM. It has challanges due to the fact that it starts iterating 
from $\delta = 0$, where the gradient is small. However, it has the risk of overshooting the optimal 
perturbation when the gradients grow fast outside this neighborhood. 


\subsubsection{Normalized Steepest Descent}

\subsubsection{Targetted attacks}

The aim of untargeted attacks is to change the predicted label to any alternative label 
by maximizing the loss for the actual (true) label and minimizing the loss for any alternative label. 

The aim of the targeted attacks is to change the predicted label to a particular alternative label 
by maximizing the loss for the actual (true) label and minimizing the loss for the targeted label. 


\subsubsection{Non-$l_{\infty}$ norms}

% The key high-level element to note here is that while ℓ∞ attacks lead to small noise everywhere in the image (precisely because this is the perturbation that is allowable under the ℓ∞ ball, ℓ2 attacks lead to perturbations that are more localized in the image, because we can “trade off” a larger perturbation in one point of the space for less perturbation in another point.

In the methods described above, the optimization was performed under the $l_{\infty}$ norm; however, 
the other norm types can also be used for the same task.

For example, we can transfer the task to the $l_2$ norm by creating a constraint that $x+\delta$ lies in the range [0, 1].

While working with $l_2$ norm, ,we simply project normalized steepest descent for the $l_2$ ball. 

\[\delta := P_{\epsilon}\left(\delta - \alpha \frac{\nabla_{\delta} \ell(h_{\theta}(x + \delta), y)}{\|\nabla_{\delta} \ell(h_{\theta}(x + \delta), y)\|_2}\right)\]


\subsection{Exactly solving the inner maximization (combinatorial optimization)}

We assume attacks against each and every class and determine whether an adversarial example exists in the neighbourhood of some point.

\subsubsection{Certifying robustness}

% If we want to determine, exactly, whether any adversarial example exists for a given example and , we can simply run the integer programming solution using a targeted attack for every possible alternative class label. If any of these optimization objectives have a negative solution, then there exists and adversarial example, and the optimization formulation provides it for us. In contrast, if none of the optimization objectives is negative for any target class, then the classifier has been formally certified to be robust on this example. Let’s see how to do this to verify that a smaller perturbation ball cannot change the class label.



\subsubsection{Upper and Lower bounds}

Due to the high computation complexity of the excat solution method, this approach is not a good choice since it can be barely scaled to larger networks. 

\subsection{Upper bounding the inner maximization}


The other method is Upper Bounding, which incorporates different approaches to forming an upper bound. We will discuss two apppraches: convex relaxation of the integer programming and the other is \betaased on bound propagation.

\subsubsection{Convex relaxation}

% The tricky part of the integer program we discussed earlier is the binary constraint vi∈{0,1}vi​∈{0,1}, which captures the ReLU operation. This constraint makes the problem hard to solve because it's not a convex set. Without this constraint, the problem would be much easier to solve, as it would become a linear program, which has faster solution methods.

% To make the problem easier, we can relax this constraint. Instead of requiring vivi​ to be either 0 or 1, we allow it to take on any value between 0 and 1. So, the relaxed constraint becomes 0≤vi≤10≤vi​≤1. This relaxation makes the problem more manageable because it removes the combinatorial complexity introduced by the binary constraint.


% So, by relaxing the binary constraint to allow vivi​ to take any value between 0 and 1, we're essentially making the problem easier to handle. This is a common strategy used in optimization, known as convex relaxation.

% Despite this change, the overall optimization problem remains unchanged from before. The key idea is that this relaxation helps us better understand the problem. For instance, if we solve the relaxed version of a targeted attack problem and find that the objective (the value we're trying to optimize) is still positive, it tells us something important. Specifically, it suggests that the attack won't be effective.

% Why? Because if the relaxed problem's objective is positive, it means that the relaxed set of solutions is larger than the original set. In other words, if no "adversarial example" exists in the relaxed set, it won't exist in the original set either. So, even though we've simplified the problem by relaxing the constraints, we can still draw meaningful conclusions from it.