\section{Chapter 3 - Neural Networks}

\subsection{Aspects of Neural Networks}

The application and performance of adversarial attacks are highly relevant to neural networks.

The form of the optimization problem (inner maximization problem) remains the same:
\[\max_{\|\delta\|\leq\epsilon} \ell(h_{\theta}(x + \delta), y)\]    
What is different from the previous examples is $h(\theta)$, which now represents a neural network.

The complexity of neural networks' architecture makes them more challenging to be robust, while simultaneously rendering them susceptible to adversarial attacks.

First of all, loss surfaces do not often guide to the optimal solution, while they can be too steep, which often corresponds to local optima convergence.

Secondly, the inner maximization problem for neural networks is also more challenging due to the non-convexity of the cost surface.

Moreover, it is more difficult to navigate the cost surface.\\

To further explore the topic of adversarial attacks in neural networks, let's propose that adversarial attacks are based on two main components:

\begin{enumerate}
\item The norm of the perturbation ball.
\item The optimization method used within that norm ball.\\
\end{enumerate}

Three main approaches to adversarial attacks on neural networks exist: 
\begin{enumerate}
    \item Lower Bounding the inner optimization, 
    \item Exactly solving the inner maximization (combinatorial optimization), 
    \item Upper Bound optimization
\end{enumerate}


\subsection{Lower Bounding the inner optimization}

The Lower Bound method relying on empirical solutions is considered the most common approach.

\subsubsection{The Fast Gradient Sign Method (FGSM)}

The idea of adversarial attacks via the FGSM relies on calculating an optimal perturbation 
$\delta$ that maximizes the loss function. This calculation is based on the gradient obtained through backpropagation. 
This gradient indicates how the loss function changes when we make small adjustments to $\delta$.

Based on the gradient, we adjust $\delta$ to maximize the loss function. 
\[g = \nabla_\delta l(h_\theta(x + \delta), y)\]

This is performed by adding a fraction of 
the gradient to $\delta$, scaled by a small step size parameter $\alpha$.

\[\delta = \delta +  \alpha \cdot g\]

We adjust $\delta$ based on the sign of the gradient: when $g<0$, $\delta < 0$ and when $g>0$, $\delta > 0$. 
Thus, we have \[\delta := \epsilon \cdot \text{sign}(g)\].

However, we also need to stick to the constraints. So, we might need to project $\delta$ back into a feasible region after adjusting it.

The  $l_{\infty}$ norm ball, which selects the maximum absolute value among the elements in the set, is well-suited for this task.

We must ensure $\delta$ remains within the norm ball: $||\delta||_\infty \leq \epsilon$. 
Therefore, we project $\delta$ back into the norm ball after each adjustment.


So, then we project $\delta$ again into the norm ball.  


FGSM is specifically designed to attack under $l_\infty$ norm. 


\subsubsection{Projected Gradient Descent}

An iterative method that ensures to adjust the perturbation $\delta$ in the direction that maximizes the loss function 
while ensuring it stays within the specified constraints.

Both FGSM and PGD generate adversarial examples that are optimal since they maximize the loss function.
While FGSM is a straightforward, one-step process, which takes a single step in the direction of the goal,
PGD iteratively updates the perturbation to generate adversarial examples. 

Thus, we can assume PGD as an iterative FGSM, which iterates over multiple steps.

\[\delta := \text{P}(\delta + \alpha \nabla_{\delta} \ell(h_\theta(x + \delta), y))\]

$P$ is the projection of the value onto the ball norm.

Although PGD can find the optima more effectively than FGSM is is proved to be slower. PGD has challanges due to the fact that it starts iterating 
from $\delta = 0$, where the gradient is small. However, it has the risk of overshooting the optimal 
perturbation when the gradients grow fast outside this neighborhood. 


\subsubsection{Targetted attacks}

The aim of untargeted attacks is to change the predicted label to any alternative label 
by maximizing the loss for the actual (true) label and minimizing the loss for any alternative label. 

The aim of the targeted attacks is to change the predicted label to a particular alternative label 
by maximizing the loss for the actual (true) label and minimizing the loss for the targeted label. 


\subsubsection{Non-$l_{\infty}$ norms}

Note: projecting onto the norm ball means clipping values of $\delta$ to lie within $[-\epsilon, \epsilon]$

In the methods described above, the optimization was performed under the $l_{\infty}$ norm; however, 
the other norm types can also be used for the same task.

For example, we can transfer the task to the $l_2$ norm by creating a constraint that $x+\delta$ lies in the range [0, 1].

While working with $l_2$ norm, we simply project normalized steepest descent for the $l_2$ ball. 

\[\delta := P_{\epsilon}\left(\delta - \alpha \frac{\nabla_{\delta} \ell(h_{\theta}(x + \delta), y)}{\|\nabla_{\delta} \ell(h_{\theta}(x + \delta), y)\|_2}\right)\]

The key behind the $l_\infty$ and $l_2$ norms is the fact that while $l_\infty$ attacks are distributed smoothly in the image, 
$l_2$ attacks are typically localized in the specific area of the image. 




\subsection{Exactly solving the inner maximization (combinatorial optimization)}

We assume attacks against each and every class and determine whether an 
adversarial example exists in the neighbourhood of some point.

In this method, we do not actually solve the inner maximization problem, but we can determine exactly whether or not an adversarial
example exists within a certain radius. 



\subsubsection{Upper and Lower bounds}

Due to the high computation complexity of the excat solution method, this approach is not a good choice since it can be barely scaled to larger networks. 

\subsection{Upper bounding the inner maximization}


The other method is Upper Bounding, which incorporates different approaches to forming an upper bound. We will discuss two apppraches: convex relaxation of the integer programming and the other 
is based on bound propagation.

\subsubsection{Convex relaxation}

The challenge that arose in the previous method is the binary constraint $v_i \in {0, 1}$, which 
captures the ReLU operation. This problem is hard to solve since the set is non-convex. 
So, we can ease the problem by relaxing the constraints and dealing with a convex set instead.

Instead of requiring $v_i$ to be 0 or 1, we allow it to take fractional values, i.e.:

\[ 0  \leq v_i \leq 1\]


Despite this change, the overall optimization problem remains unchanged from before.

The key idea is that this relaxation helps us better understand the problem. 

For instance, if we solve the relaxed version of a targeted attack problem and
find that the objective (the value we're trying to optimize) is still positive, 
it tells us something important. Specifically, it suggests that the attack won't be effective.


If no "adversarial example" exists in the relaxed set, it won't exist in the original 
set either. So, even though we've simplified the problem by relaxing the constraints, we 
can still draw meaningful conclusions from it.